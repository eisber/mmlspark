{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook assumes you have an Accumulo cluster running and have a shaded jar file with accumulo dependencies.\n",
    "- To setup a simple Accumulo cluster: https://github.com/apache/fluo-uno\n",
    "- To build a shaded jar with accumulo dependencies: https://github.com/apache/accumulo-examples/tree/master/spark\n",
    "\n",
    "Adding the shaded jar after the notebook is running does not work, it needs to be added prior to starting jupyter\n",
    "\n",
    "```\n",
    "SPARK_OPTS=\"--jars ~/repos/accumulo-examples/spark/target/accumulo-spark-shaded.jar\" jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of mmlspark_2.11-0.17+108-eef7302b+20190818-2038.jar\n"
     ]
    }
   ],
   "source": [
    "// Adding the MMLSpark JAR dynamically does work\n",
    "%AddJar file:///~/repos/mmlspark/target/scala-2.11/mmlspark_2.11-0.17+108-eef7302b+20190818-2038.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "import org.apache.accumulo.core.client.{Accumulo, AccumuloClient, BatchWriter}\n",
    "import org.apache.accumulo.core.data.{Key, Mutation, Value}\n",
    "import org.apache.accumulo.core.security.Authorizations\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{DataType, MetadataBuilder, StringType, StructType, StructField}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accumuloClientProperties: String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def accumuloClientProperties = \"/home/scott/repos/fluo-uno/install/accumulo-2.0.0/conf/accumulo-client.properties\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Accumulo Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputTable = spark_example_input\n",
       "rootPath = /spark_example\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/spark_example"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputTable = \"spark_example_input\";\n",
    "val rootPath = new Path(\"/spark_example/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "props = {auth.type=password, auth.principal=root, instance.zookeepers=localhost:2181, instance.name=uno, auth.token=secret}\n",
       "client = org.apache.accumulo.core.clientImpl.ClientContext@7500f36f\n",
       "hdfs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1688114584_96, ugi=scott (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1688114584_96, ugi=scott (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create accumulo client\n",
    "val props = Accumulo.newClientProperties().from(accumuloClientProperties).build();\n",
    "val client = Accumulo.newClient().from(props).build()\n",
    "\n",
    "val hdfs = FileSystem.get(new Configuration());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// cleanup hdfs root path\n",
    "if (hdfs.exists(rootPath)) {\n",
    "  hdfs.delete(rootPath, true);\n",
    "}\n",
    "\n",
    "// remove accumulo table \n",
    "if (client.tableOperations().exists(inputTable)) {\n",
    "  client.tableOperations().delete(inputTable);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create table\n",
    "client.tableOperations().create(inputTable);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batchWriter = org.apache.accumulo.core.clientImpl.BatchWriterImpl@76640258\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.accumulo.core.clientImpl.BatchWriterImpl@76640258"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// write data to input table\n",
    "val batchWriter = client.createBatchWriter(inputTable)\n",
    "for (i <- 0 until 100) {\n",
    "  val m = new Mutation(f\"$i%03d\")\n",
    "  m.at().family(\"cf1\").qualifier(\"cq1\").put(\"\" + i);\n",
    "  batchWriter.addMutation(m);\n",
    "}\n",
    "batchWriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scanner = org.apache.accumulo.core.clientImpl.ScannerImpl@3689a34f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.accumulo.core.clientImpl.ScannerImpl@3689a34f"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scanner = client.createScanner(inputTable, Authorizations.EMPTY)\n",
    "scanner.fetchColumnFamily(\"cf1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 cf1:cq1 [] 1566146448794 false=0\n",
      "001 cf1:cq1 [] 1566146448794 false=1\n",
      "002 cf1:cq1 [] 1566146448794 false=2\n",
      "003 cf1:cq1 [] 1566146448794 false=3\n",
      "004 cf1:cq1 [] 1566146448794 false=4\n",
      "005 cf1:cq1 [] 1566146448794 false=5\n",
      "006 cf1:cq1 [] 1566146448794 false=6\n",
      "007 cf1:cq1 [] 1566146448794 false=7\n",
      "008 cf1:cq1 [] 1566146448794 false=8\n",
      "009 cf1:cq1 [] 1566146448794 false=9\n"
     ]
    }
   ],
   "source": [
    "scanner.zipWithIndex.foreach { case(e, i) => if (i < 10) println(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark - Accumulo Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@3a37feb6\n",
       "sparkSession = org.apache.spark.sql.SparkSession@796ecf79\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@796ecf79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// reload Spark session with config\n",
    "\n",
    "val conf = new SparkConf()\n",
    "conf.setAppName(\"Test\")\n",
    "// KryoSerializer is needed for serializing Accumulo Key when partitioning data for bulk import\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "// conf.set(\"spark.sql.legacy.replaceDatabricksSparkAvro.enabled\", \"false\")\n",
    "conf.registerKryoClasses(Array(classOf[Key], classOf[Value], classOf[Properties]))\n",
    "\n",
    "val sparkSession = SparkSession.builder().config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "properties = Map(auth.type -> password, auth.principal -> root, table -> spark_example_input, instance.zookeepers -> localhost:2181, instance.name -> uno, auth.token -> secret)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(auth.type -> password, auth.principal -> root, table -> spark_example_input, instance.zookeepers -> localhost:2181, instance.name -> uno, auth.token -> secret)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props.put(\"table\", inputTable)\n",
    "val properties = props.asScala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema = StructType(StructField(f1,StringType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "makeField: (fieldName: String, columnFamily: String, columnQualifier: String, dataType: org.apache.spark.sql.types.DataType, nullable: Boolean)org.apache.spark.sql.types.StructField\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\"type\":\"struct\",\"fields\":[{\"name\":\"f1\",\"type\":\"string\",\"nullable\":false,\"metadata\":{\"cf\":\"cf1\",\"cq\":\"cq1\"}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeField(fieldName: String, columnFamily: String, columnQualifier: String, dataType: DataType, nullable: Boolean): StructField = {\n",
    "    new StructField(fieldName, \n",
    "                    dataType, \n",
    "                    nullable, \n",
    "                    new MetadataBuilder()\n",
    "                        .putString(\"cf\", columnFamily)\n",
    "                        .putString(\"cq\", columnQualifier)\n",
    "                        .build())\n",
    "}\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(makeField(\"f1\", \"cf1\", \"cq1\", StringType, false))\n",
    "\n",
    "schema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.avro.Schema$Field.<init>(Ljava/lang/String;Lorg/apache/avro/Schema;Ljava/lang/String;Ljava/lang/Object;)V\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$$anonfun$2.apply(AccumuloInputPartitionReader.scala:50)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$$anonfun$2.apply(AccumuloInputPartitionReader.scala:49)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$.catalystSchemaToAvroSchema(AccumuloInputPartitionReader.scala:49)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader.<init>(AccumuloInputPartitionReader.scala:102)\n",
       "\tat com.microsoft.ml.spark.accumulo.PartitionReaderFactory.createPartitionReader(AccumuloDataSourceReader.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$$anonfun$2.apply(AccumuloInputPartitionReader.scala:50)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$$anonfun$2.apply(AccumuloInputPartitionReader.scala:49)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$.catalystSchemaToAvroSchema(AccumuloInputPartitionReader.scala:49)\n",
       "\tat com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader.<init>(AccumuloInputPartitionReader.scala:102)\n",
       "\tat com.microsoft.ml.spark.accumulo.PartitionReaderFactory.createPartitionReader(AccumuloDataSourceReader.scala:42)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
       "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:745)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:704)\n",
       "  ... 48 elided\n",
       "Caused by: java.lang.NoSuchMethodError: org.apache.avro.Schema$Field.<init>(Ljava/lang/String;Lorg/apache/avro/Schema;Ljava/lang/String;Ljava/lang/Object;)V\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$$anonfun$2.apply(AccumuloInputPartitionReader.scala:50)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$$anonfun$2.apply(AccumuloInputPartitionReader.scala:49)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader$.catalystSchemaToAvroSchema(AccumuloInputPartitionReader.scala:49)\n",
       "  at com.microsoft.ml.spark.accumulo.AccumuloInputPartitionReader.<init>(AccumuloInputPartitionReader.scala:102)\n",
       "  at com.microsoft.ml.spark.accumulo.PartitionReaderFactory.createPartitionReader(AccumuloDataSourceReader.scala:42)\n",
       "  at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sparkSession\n",
    "    .read\n",
    "    .format(\"com.microsoft.ml.spark.accumulo\")\n",
    "    .options(properties)\n",
    "    .schema(schema)\n",
    "    .load()\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparkSession.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
